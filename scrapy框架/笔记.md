# scrapy模板

## 一.作用

1.scrapy封装了高性能的持久化存储，异步数据下载，高性能的数据解析，分布式。

## 二.环境的安装

1.mac 和 linux只需要 pip install scrapy

2.win需要

	* pip install wheel
	* 下载  twsited 地址 http://www.lfd.uci.edu/~gohlke/pythonlibs/#twsited
	* 安装 twisted：pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
	* pip install pywin32
	* pip install scrapy

测试：在终端输入scrapy指令，没有报错即可

## 三.使用scrapy

1.通过终端指令创建工程

​	scrapy startproject xxxPro :创建名为xxxPro的工程

2.创建爬虫源文件

* 先cd进入项目
* 再在终端输入 scrapy genspider filename www.XXX.com
* 创建名为filename 的初始url为www.xxx.com
* 注意：setting文件保存配置
* 注意：spider文件夹放爬虫文件

3.运行爬虫

​	scrapy crawl spidername #运行名为spider的文件的爬虫文件，在终端中

4.生成的爬虫文件讲解

```python
import scrapy

class SpiderscriptSpider(scrapy.Spider):  #scrapy spider为自动封装的spider父类
    name = 'spiderscript'  #爬虫文件的名称，有多个爬虫文件时不能重复
    #allowed_domains = ['www.XXX.com']  允许的域名：限定start_urls那些域名能被访问，在这个列表中的才能请求发送，
    #一般不使用
    start_urls = ['http://www.XXX.com/']  #该列表中的url会被自动的请求发送

    def parse(self, response):  #用于数据解析的函数，调用的个数与请求的次数一致
        print(response)

```

5.配置文件的配置

![image](https://user-images.githubusercontent.com/88570271/143467538-20dcad90-c0ad-4b1f-b1cc-9c08fdc5cdeb.png)

（1）遵从robots协议导致爬不到数据

![image](https://user-images.githubusercontent.com/88570271/143467581-fd87c79b-157a-4a6a-b250-7b4f8a09dd8e.png)

==(2)将True改为False就能关闭协议==

==(3)设置配置文件不显示日志信息==

```python
scrapy crawl spiderscript --nolog  执行爬虫但是不输出日志，但不会输出错误信息
LOG-LEVEL = 'ERROR'  建议在配置文件任意位置添加该语句，表示只显示错误信息
```

(4)User-Agent手动修改好，初始状态为注释

![image](https://user-images.githubusercontent.com/88570271/143467639-ddaa918d-9d56-4772-8b72-f82a18de4bfa.png)

(5)response.xpath()返回的并不是一个字符串，而是一个select对象

![image](https://user-images.githubusercontent.com/88570271/143467769-a98f16ca-0626-466c-81dd-5c3830cb0ea1.png)

可以使用extract（）方法 列表也可以调用 extract_rist（）可以将第一个转化为字符串

![image](https://user-images.githubusercontent.com/88570271/143467796-42851535-6ea0-4fc6-b981-48cea730cd7c.png)

## 四.持久化存储

### 1.基于终端指令的存储

	* 要求 - 只可以将parse方法返回值存储到本地文本文件中
  * ![image](https://user-images.githubusercontent.com/88570271/143467830-955039e1-447c-4624-9c67-da3cd348df91.png)
	* 终端代码 - scrapy crawl spiderscript -o “pathdata.csv“ #path为文件存储的路径
	* 注意 - 基于终端存储只能存储到以下文件格式中
	* ![image](https://user-images.githubusercontent.com/88570271/143467869-a83d7cbc-f06d-4a72-9640-98bda65ef2f2.png)

### 2.基于管道的持久化存储

 * 编码流程

 * * 数据解析

    * 在item类中定义相关的属性

    * ![image](https://user-images.githubusercontent.com/88570271/143467930-ceccdf5b-66e7-4b38-ad82-0b0b42d9c17e.png)

    * 注意：必须使用Field方法定义属性

    * 将解析到的数据封装存储到item类型的对象,才能使用定义好的item类

    * `要记得导入item类！！！！！`

    * ![image](https://user-images.githubusercontent.com/88570271/143467966-4607af9c-12f9-4a50-9492-38d3cb30fdcd.png)

    * 将item类对象提交给管道进行持久化存储的操作 每提交一次就运行一次管道方法

    * ```python
        class SpiderCompanyPipeline(object):
            fp= None
            #重写父类的方法 该方法只会在开始爬虫的时候调用一次
            def open_spider(self,spider):
                print("开始爬虫")
                self.fp = open('./qiubai.txt','w',encoding="utf-8")
            def process_item(self, item, spider):  #该方法可以接收到提交过来的item对象
                author = item['author']
                content = item['content']
                self.fp.write(author+":"+content+'\n')
        
                return item
            def close_spider(self,spider):  #重写父类方法，只在爬虫结束的时候调用一次
                print("结束爬虫")
                self.fp.close()
        ```

    * 

    * 在配置文件中开启管道机制

    * ![image](https://user-images.githubusercontent.com/88570271/143468046-d91c7e9f-c79d-403b-86a6-5823602dfd3d.png)

    * 300表示的优先级，数字越小优先级越高

3.数据库存储：

* 首先要连接数据库
* ![image](https://user-images.githubusercontent.com/88570271/143468080-8ba77cf1-4c28-4a5c-9f9a-525724f41d2b.png)
* 然后定义游标对象
* ![image](https://user-images.githubusercontent.com/88570271/143468112-52390805-87c2-49c8-becf-4d6d21d7e383.png)
* 利用游标对象创建根据相应的接口表格
* ![image](https://user-images.githubusercontent.com/88570271/143468158-a5bee4d3-7237-4faa-9340-4899ab10c94e.png)
* 定义插入命令字符串（创建表格也可以使用字符串命令）
* ![image](https://user-images.githubusercontent.com/88570271/143468207-68757e55-c798-4bf5-81a0-110397d36efc.png)
* 执行命令
* ![image](https://user-images.githubusercontent.com/88570271/143468247-c11e271a-1a95-47bb-bf89-341ec8147383.png)
* `注意：第一个参数时要执行命令的字符串，第二个参数是插入值的元组 最好使用try语句来输出错误并回滚`
* 不要忘记使用连接数据库的对象每次提交数据
* 报错笔记：`将创建表格的语句写到process_item中导致每次调用该函数都会删除原来的表格并创建新的表格 解决放法：将语句写到open_spider中`
